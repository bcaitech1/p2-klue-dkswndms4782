{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RmcmqTvs1_T"
   },
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5ZMzORj6Xxn"
   },
   "source": [
    "라이브러리 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apt-get install g++\n",
    "# !pip install pororo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/SKTBrain/KoBERT\n",
    "# 여기 koBert랑 KoGPT2가 있는데 같이 사용해서 앙상블하면,,,좋지않을까,,,\n",
    "# 아니면 다국어 모델?도 좋을거같은데 \n",
    "# ERNIE, ELECTRA, GPT-2 이런거...? ㅎㅎ,,,,\n",
    "# KoGPT2에 토크나이저만 바꾸는건 에반가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcI3nARqs9qg"
   },
   "source": [
    "라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ETROhbNxsuXQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tarfile\n",
    "import pickle as pickle\n",
    "from tqdm import tqdm\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup, LambdaLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning import Trainer\n",
    "import os\n",
    "from glob import glob\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "# from pororo import Pororo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mahnyujin\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.27 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.26<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">distinctive-resonance-54</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ahnyujin/Pstage2_KLUE\" target=\"_blank\">https://wandb.ai/ahnyujin/Pstage2_KLUE</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ahnyujin/Pstage2_KLUE/runs/2izfnoa6\" target=\"_blank\">https://wandb.ai/ahnyujin/Pstage2_KLUE/runs/2izfnoa6</a><br/>\n",
       "                Run data is saved locally in <code>/opt/ml/code/wandb/run-20210420_075300-2izfnoa6</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login()\n",
    "wandb.init(project = 'Pstage2_KLUE', reinit = True)\n",
    "wandb.run.name = 'koBert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "seed = 7\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcobKDe9tAuQ"
   },
   "source": [
    "GPU 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "i8v0khrlswNx"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hZw_ITPtCgp"
   },
   "source": [
    "kobert 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nhsub2pBsx1q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9_lv7GMtE1_"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "5mr-nvcjOzLF"
   },
   "outputs": [],
   "source": [
    "def load_data(dataset_dir):\n",
    "    with open('/opt/ml/input/data/label_type.pkl', 'rb') as f:\n",
    "        label_type = pickle.load(f)\n",
    "    dataset = pd.read_csv(dataset_dir, delimiter='\\t', header=None)\n",
    "    dataset = preprocessing_dataset(dataset, label_type)\n",
    "    return dataset\n",
    "\n",
    "def preprocessing_dataset(dataset, label_type):\n",
    "    label = []\n",
    "    for i in dataset[8]:\n",
    "        if i == 'blind':\n",
    "            label.append(100)\n",
    "        else:\n",
    "            label.append(label_type[i])\n",
    "    out_dataset = pd.DataFrame({'sentence':dataset[1],'entity_01':dataset[2],'entity_02':dataset[5],'label':label,})\n",
    "    return out_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xkI-E7PauxGq"
   },
   "outputs": [],
   "source": [
    "dataset_path = r\"/opt/ml/input/data/train/train.tsv\"\n",
    "\n",
    "dataset = load_data(dataset_path)\n",
    "\n",
    "dataset['sentence'] = dataset['entity_01'] + ' [SEP] ' + dataset['entity_02'] + ' [SEP] ' + dataset['sentence']\n",
    "# dataset['sentence'] = dataset['sentence'] +  '이 문장에서 ' + dataset['entity_01'] + '과 ' + dataset['entity_02'] + \"은 어떤 관계일까?[SEP]\"\n",
    "                                                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YIE_tnYq6AYL"
   },
   "outputs": [],
   "source": [
    "# dataset[['sentence', 'label']].to_csv(\"/opt/ml/input/data/train/train.txt\", sep='\\t', index=False)\n",
    "train, vali = train_test_split(dataset, test_size=0.1, random_state=42)\n",
    "train[['sentence','label']].to_csv(\"/opt/ml/input/data/train/train_train.txt\", sep='\\t',index=False) # sep='\\t',\n",
    "vali[['sentence','label']].to_csv(\"/opt/ml/input/data/train/train_vali.txt\",  sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "2tI-jupiCwpE"
   },
   "outputs": [],
   "source": [
    "# dataset = nlp.data.TSVDataset(\"/opt/ml/input/data/train/train.txt\", field_indices=[0,1], num_discard_samples=1, allow_missing = True)\n",
    "dataset_train = nlp.data.TSVDataset(\"/opt/ml/input/data/train/train_train.txt\", field_indices=[0,1], num_discard_samples=1, allow_missing = True)\n",
    "dataset_vali = nlp.data.TSVDataset(\"/opt/ml/input/data/train/train_vali.txt\", field_indices=[0,1], num_discard_samples=1,allow_missing = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ca54j41sN-0L"
   },
   "outputs": [],
   "source": [
    "# tokenizer = get_tokenizer()\n",
    "# tok = nlp.data.BERTSPTokenizer(tokenizer,vocab = vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=True,\n",
    "    strip_accents=False, \n",
    "    lowercase=False,\n",
    "    wordpieces_prefix=\"##\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./t-vocab.txt']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.train(\n",
    "    files = \"/opt/ml/input/data/train/train.txt\",\n",
    "    # min_frequency=2,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"[ENT]\", \"[\\ENT]\"],\n",
    "    limit_alphabet=1000,\n",
    "    wordpieces_prefix=\"##\",\n",
    ")\n",
    "tokenizer.save_model(\"./\",\"t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from konlpy.tag import Mecab\n",
    "# mecab_tokenizer = Mecab(dicpath=r\"C:\\mecab\\mecab-ko-dic\").morphs\n",
    "\n",
    "# total_morph=[]\n",
    "# for idx,i in enumerate(dataset_train):\n",
    "#     # 문장단위 mecab 적용\n",
    "#     morph_sentence= []\n",
    "#     count = 0\n",
    "#     for token_mecab in mecab_tokenizer(i[0]):\n",
    "#         token_mecab_save = token_mecab\n",
    "#         if count > 0:\n",
    "#             token_mecab_save = \"##\" + token_mecab_save  # 앞에 ##를 부친다\n",
    "#             morph_sentence.append(token_mecab_save)\n",
    "#         else:\n",
    "#             morph_sentence.append(token_mecab_save)\n",
    "#             count += 1\n",
    "#     # 문장단위 저장\n",
    "#     dataset_train[idx][i] = morph_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "eRRaHwF_C28c"
   },
   "outputs": [],
   "source": [
    "# class BERTDataset(Dataset):\n",
    "#     def __init__(self, dataset, sent_idx, label_idx, tokenizer, max_len, pad, pair):\n",
    "#         #transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "#         self.sentences = [tokenizer.encode(i[sent_idx]) for i in dataset]\n",
    "#         self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "#     def __getitem__(self, i):\n",
    "#         return (self.sentences[i] + (self.labels[i]))\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, tokenizer, max_len, pad, pair):\n",
    "        self.sentence = [tokenizer.encode(i[sent_idx]) for i in dataset]\n",
    "        self.input_ids = []\n",
    "        self.type_ids = []\n",
    "        self.attention_mask = []\n",
    "        self.special_tokens_mask = []\n",
    "        for i in self.sentence:\n",
    "            if len(i.ids) < max_len:\n",
    "                self.input_ids.append(np.pad(i.ids,(0,max_len - len(i.ids)), constant_values = 0))\n",
    "                self.type_ids.append(np.pad(i.type_ids,(0,max_len - len(i.ids)), constant_values = 0))\n",
    "                self.attention_mask.append(np.pad(i.attention_mask,(0,max_len - len(i.ids)), constant_values = 0))\n",
    "                self.special_tokens_mask.append(np.pad(i.special_tokens_mask,(0,max_len - len(i.ids)), constant_values = 0))\n",
    "            elif len(i.ids) >= max_len:\n",
    "                self.input_ids.append(i.ids[:128])\n",
    "                self.type_ids.append(i.type_ids[:128])\n",
    "                self.attention_mask.append(i.attention_mask[:128])\n",
    "                self.special_tokens_mask.append(i.special_tokens_mask[:128])\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return ([self.input_ids[i]] + [self.type_ids[i]]\n",
    "                + [self.attention_mask[i]] + [self.special_tokens_mask[i]]\n",
    "                                + [self.labels[i]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "4BKznxZotPrl"
   },
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "batch_size = 32\n",
    "warmup_ratio = 0.01\n",
    "num_epochs = 20\n",
    "max_grad_norm = 1\n",
    "log_interval = 50\n",
    "learning_rate = 25e-6\n",
    "\n",
    "wandb.config.update({\n",
    "    \"model\" : \"koBERT\",\n",
    "    \"loss\" : \"focal_loss\",\n",
    "    \"max_len\" : 128,\n",
    "    \"batch_size\" : 32,\n",
    "    \"warmup_ratio\" : 0.01,\n",
    "    \"num_epochs\" : 20,\n",
    "    \"max_grad_norm\" : 1,\n",
    "    \"log_interval\" : 50,\n",
    "    \"learning_rate\" : 25e-6,\n",
    "    \"dr_rate\" : 0.5\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "WtW5knVCC6ZC"
   },
   "outputs": [],
   "source": [
    "#data = BERTDataset(dataset, 0, 1,tok, max_len, True, False)\n",
    "data_train = BERTDataset(dataset_train, 0, 1,tokenizer, max_len, True, False)\n",
    "data_vali = BERTDataset(dataset_vali, 0, 1,tokenizer, max_len, True, False)\n",
    "# print(data_train[0])\n",
    "# [0][0] : token_ids : Token Embedding을 생성하는데 필요\n",
    "# [0][1] : valid_len : Positional Embedding을 위한것\n",
    "# [0][2] : token_types : Sentence Embedding\n",
    "# token_ids, valid_length, segment_ids, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train[0]\n",
    "# input_ids[i] + type_ids[i] \n",
    "# + attention_mask[i] + special_tokens_mask[i]\n",
    "# labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "spDs0h8tC7fX"
   },
   "outputs": [],
   "source": [
    "#train_dataloader = torch.utils.data.DataLoader(data, batch_size=batch_size, num_workers=5)\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "vali_dataloader = torch.utils.data.DataLoader(data_vali, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0I1L7EVtShS"
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "eR9IqXuStUbL"
   },
   "outputs": [],
   "source": [
    "# class BERTClassifier(nn.Module):\n",
    "#     def __init__(self,\n",
    "#                  bert,\n",
    "#                  hidden_size = 768,\n",
    "#                  num_classes = 42,\n",
    "#                  dr_rate=None,\n",
    "#                  params=None):\n",
    "#         super(BERTClassifier, self).__init__()\n",
    "#         self.bert = bert\n",
    "#         self.dr_rate = dr_rate \n",
    "#         self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "#         if dr_rate:\n",
    "#             self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "#     def gen_attention_mask(self, token_ids, valid_length):\n",
    "#         attention_mask = torch.zeros_like(token_ids)\n",
    "#         for i, v in enumerate(valid_length):\n",
    "#             attention_mask[i][:v] = 1\n",
    "#         return attention_mask.float()\n",
    "\n",
    "#     def forward(self, token_ids, valid_length, segment_ids):\n",
    "#         attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "#         _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "#         if self.dr_rate:\n",
    "#             out = self.dropout(pooler)\n",
    "#         return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes = 42,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "\n",
    "    def forward(self, token_ids, segment_ids, attention_mask):\n",
    "        _, pooler = self.bert(input_ids = token_ids.long(), token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "piJyyUoutWWt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<wandb.wandb_torch.TorchGraph at 0x7fad0df6e990>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertmodel.resize_token_embeddings(len(tokenizer.get_vocab()))\n",
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "TqaRnWqwtXii"
   },
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "YYExV_Uwqdpi"
   },
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes=42, smoothing=0.2, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "    \n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None,\n",
    "                 gamma=2., reduction='mean'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "        log_prob = F.log_softmax(input_tensor, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            ((1 - prob) ** self.gamma) * log_prob,\n",
    "            target_tensor,\n",
    "            weight=self.weight,\n",
    "            reduction=self.reduction\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "SvLPsHAMtYp4"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = FocalLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "wJrYbrK5taVC"
   },
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "4PDk3f8ctasE"
   },
   "outputs": [],
   "source": [
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "7uxhVAqWtcbJ"
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASK6KHOTtd2H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 3.581865072250366 train acc 0.0625\n",
      "epoch 1 batch id 51 loss 2.267670154571533 train acc 0.38112745098039214\n",
      "epoch 1 batch id 101 loss 1.9262659549713135 train acc 0.43285891089108913\n",
      "epoch 1 batch id 151 loss 2.1379289627075195 train acc 0.45695364238410596\n",
      "epoch 1 batch id 201 loss 1.7742887735366821 train acc 0.46455223880597013\n",
      "epoch 1 batch id 251 loss 1.4959793090820312 train acc 0.4660109561752988\n",
      "**************************************************\n",
      "epoch 1 train acc 0.46788877952755903\n",
      "**************************************************\n",
      "9\n",
      "**************************************************\n",
      "epoch 1 test acc 0.5032327586206896\n",
      "**************************************************\n",
      "==================================================\n",
      "best_acc : 0.5032327586206896\n",
      "==================================================\n",
      "epoch 2 batch id 1 loss 2.290195941925049 train acc 0.40625\n",
      "epoch 2 batch id 51 loss 1.913037657737732 train acc 0.49019607843137253\n",
      "epoch 2 batch id 101 loss 1.8338284492492676 train acc 0.4879331683168317\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    best_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, segment_ids, attention_mask, special_tokens_mask,  label) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        try:\n",
    "            token_ids = token_ids.long().to(device)\n",
    "        except:\n",
    "            continue\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        label = label.long().to(device)\n",
    "        attention_mask = attention_mask.float().to(device)\n",
    "        special_tokens_mask = special_tokens_mask.long().to(device)\n",
    "        out = model(token_ids, segment_ids, attention_mask)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "            wandb.log({\n",
    "                \"loss\" :  loss.data.cpu().numpy(),\n",
    "                \"train acc \" : train_acc / (batch_id+1)\n",
    "            })\n",
    "    print('*'*50)\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    wandb.log({\n",
    "        \"train acc \" :  train_acc / (batch_id+1)\n",
    "    })\n",
    "    print('*'*50)\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, segment_ids, attention_mask, special_tokens_mask,  label) in enumerate(vali_dataloader):\n",
    "        try:\n",
    "            token_ids = token_ids.long().to(device)\n",
    "        except:\n",
    "            print(batch_id)\n",
    "            continue\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        attention_mask = attention_mask.float().to(device)\n",
    "        special_tokens_mask = special_tokens_mask.long().to(device)\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, segment_ids, attention_mask)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print('*'*50)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "    wandb.log({\n",
    "        \"test acc\" : test_acc / (batch_id+1)\n",
    "    })\n",
    "    print('*'*50)\n",
    "    if test_acc >= best_acc:\n",
    "        best_acc = test_acc\n",
    "        print('='*50)\n",
    "        print(f\"best_acc : {best_acc / (batch_id+1)}\")\n",
    "        print('='*50)\n",
    "        torch.save(model.state_dict(), \"/opt/ml/model/model_state_dict.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [32,128]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7ASgrTpfdZh"
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Siuxwi6SdiTW"
   },
   "outputs": [],
   "source": [
    "dataset_path = r\"/opt/ml/input/data/test/test.tsv\"\n",
    "\n",
    "dataset = load_data(dataset_path)\n",
    "\n",
    "dataset['sentence'] = dataset['entity_01'] + ' [SEP] ' + dataset['entity_02'] + ' [SEP] ' + dataset['sentence']\n",
    "# dataset['sentence'] = dataset['sentence'] +  '이 문장에서 ' + dataset['entity_01'] + '과 ' + dataset['entity_02'] + \"은 어떤 관계일까?[SEP]\"\n",
    "dataset[['sentence','label']].to_csv(\"/opt/ml/input/data/test/test.txt\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset['sentence'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPfoO4ym6AYU"
   },
   "outputs": [],
   "source": [
    "dataset_test = nlp.data.TSVDataset(\"/opt/ml/input/data/test/test.txt\", field_indices=[0,1], num_discard_samples=1)\n",
    "\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=10, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g3TFf_YgtjDG"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"/opt/ml/model/model_state_dict.pt\"))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "Predict = []\n",
    "\n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length = valid_length\n",
    "    label = label.long().to(device)\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "    _, predict = torch.max(out,1)\n",
    "    Predict.extend(predict.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_aV-Fgpffp4s"
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame(Predict, columns=['pred'])\n",
    "output.to_csv('/opt/ml/result/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "P2-KLUE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
