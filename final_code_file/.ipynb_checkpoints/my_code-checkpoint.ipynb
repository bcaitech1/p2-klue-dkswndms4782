{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c48b2ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, BertConfig,BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2da664c",
   "metadata": {},
   "source": [
    "# 1.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "801e15b6-589d-4911-a244-9faea01e16e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 구성.\n",
    "class RE_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_datasest,labels):\n",
    "        self.tokenized_dataset = tokenized_datasest\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in     self.tokenized_dataset.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 처음 불러온 tsv 파일을 원하는 형태의 DataFrame으로 변경 시켜줌\n",
    "# 변경한 DataFrame 형태는 vaseline code description 이미지를 참고해주세요.\n",
    "def preprocessing_dataset(dataset, label_type):\n",
    "    label = []\n",
    "    for i in dataset[8]:\n",
    "        if i == 'blind':\n",
    "            label.append(100)\n",
    "        else:\n",
    "            label.append(label_type[i])\n",
    "        out_dataset = pd.DataFrame({'sentence' : dataset[1], 'entity_01' : dataset[2], 'entity_02':dataset[5], 'label' : label,})\n",
    "        return out_dataset\n",
    "\n",
    "    def load_data(dataset_dir):\n",
    "        # load label_type, classes\n",
    "        with open('/opt/ml/input/data/label_type.pkl', 'rb') as f:\n",
    "            label_type = pickle.load(f)\n",
    "        # load dataset\n",
    "        dataset = pd.read_csv(dataset_dir, delimiter = '\\t', header = None)\n",
    "        # preprocessing dataset\n",
    "        dataset = preprocessing_dataset(dataset, label_type)\n",
    "        return dataset\n",
    "# bert input을 위한 tokenizing.\n",
    "# tip! 다양한 종류의 tokenizer와 special token들을 활용하는 것으로도 새로운 시도를 해볼 수 있습니다.\n",
    "# baseline code에서는 2가지 부분을 활용했습니다.\n",
    "def tokenized_dataset(dataset, tokenizer):\n",
    "    concat_entity = []\n",
    "    for e01, e02 in zip(dataset['entoty_01'], dataset['entity_02']):\n",
    "        temp = ''\n",
    "        temp = e01 + '[SEP]' + e02\n",
    "        concat_entity.append(temp)\n",
    "    tokenized_sentences = tokenizer(\n",
    "        concat_entity,\n",
    "        list(dataset['sentence']),\n",
    "        return_tensors = 'pt',\n",
    "        padding = True,\n",
    "        truncation = True,\n",
    "        max_length = 100,\n",
    "        add_special_tokens = True,\n",
    "    )\n",
    "    return tokenized_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9115ba4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   101,   9167,  15001,  ...,      0,      0,      0],\n",
      "        [   101,   9311,  16323,  ...,      0,      0,      0],\n",
      "        [   101,  68495,  37905,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [   101,   9328, 118782,  ...,    123, 101322,    102],\n",
      "        [   101,  49780,  16617,  ...,      0,      0,      0],\n",
      "        [   101,   9730,  30858,  ...,      0,      0,      0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# tokenizer가 정확히 뭐하는 앤지 한번 찾아보자\n",
    "# label_type을 붙이는게 어떤 의미인가?\n",
    "# load_data, RE_Dataset, tokenized_dataset을 어디다 쓰는지 알아보자.\n",
    "print(tokenized_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d7081c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "set_seed(210)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8ff34f",
   "metadata": {},
   "source": [
    "# 2.Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71cfbbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ml/code/load_data.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.tokenized_dataset.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2252' max='2252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2252/2252 05:47, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.647400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.243500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.159800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.164800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.171900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.135700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.113200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.064100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.993700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.892700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.845700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.869300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.850500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.813100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.800400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.717400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.617000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.651100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.637400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.514200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ml/code/load_data.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.tokenized_dataset.items()}\n",
      "/opt/ml/code/load_data.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.tokenized_dataset.items()}\n",
      "/opt/ml/code/load_data.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.tokenized_dataset.items()}\n",
      "/opt/ml/code/load_data.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.tokenized_dataset.items()}\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "    }\n",
    "\n",
    "#def train():\n",
    "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_dataset = load_data(\"/opt/ml/input/data/train/train.tsv\")\n",
    "train_label = train_dataset['label'].values\n",
    "\n",
    "# tokenizing dataset\n",
    "tokenized_train = tokenized_dataset(train_dataset, tokenizer)\n",
    "\n",
    "# make dataset for pytorch.\n",
    "RE_train_dataset = RE_Dataset(tokenized_train, train_label)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# setting model hyperparameter\n",
    "bert_config = BertConfig.from_pretrained(MODEL_NAME)\n",
    "bert_config.num_labels = 42\n",
    "model = BertForSequenceClassification(bert_config)\n",
    "model.parameters\n",
    "model.to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './results',\n",
    "    save_total_limit = 4,\n",
    "    save_steps = 500,\n",
    "    num_train_epochs = 4,\n",
    "    learning_rate = 5e-5,\n",
    "    per_device_train_batch_size = 16,\n",
    "    warmup_steps = 500,\n",
    "    weight_decay = 0.01,\n",
    "    logging_dir = './logs',\n",
    "    logging_steps = 100,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset=RE_train_dataset\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0896c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss는 어디갔지?\n",
    "# forward도 없는데 어케 학습시키는거야? optimizer는?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e1228c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27c7d703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ml/code/load_data.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.tokenized_dataset.items()}\n"
     ]
    }
   ],
   "source": [
    "def inference(model, tokenized_sent, device):\n",
    "    dataloader = DataLoader(tokenized_sent, batch_size = 40, shuffle = False)\n",
    "    model.eval()\n",
    "    output_pred = []\n",
    "\n",
    "    for i, data in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids = data['input_ids'].to(device),\n",
    "                attention_mask = data['attention_mask'].to(device),\n",
    "                token_type_ids = data['token_type_ids'].to(device)\n",
    "            )\n",
    "        logits = outputs[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        result = np.argmax(logits, axis = -1)\n",
    "\n",
    "        output_pred.append(result)\n",
    "    return np.array(output_pred).flatten()\n",
    "\n",
    "def load_test_dataset(dataset_dir, tokenizer):\n",
    "    test_dataset = load_data(dataset_dir)\n",
    "    test_label = test_dataset['label'].values\n",
    "    # tokenizing dataset\n",
    "    tokenized_test = tokenized_dataset(test_dataset, tokenizer)\n",
    "    return tokenized_test, test_label\n",
    "\n",
    "# load test dataset\n",
    "test_dataset_dir = \"/opt/ml/input/data/test/test.tsv\"\n",
    "test_dataset, test_label = load_test_dataset(test_dataset_dir, tokenizer)\n",
    "test_dataset = RE_Dataset(test_dataset, test_label)\n",
    "\n",
    "# predict answer\n",
    "pred_answer = inference(model, test_dataset, device)\n",
    "# make csv file with predicted answer\n",
    "# 아래 directory 와 columns의 형태는 지켜주시기 바랍니다. \n",
    "\n",
    "output = pd.DataFrame(pred_answer, columns = ['pred'])\n",
    "output.to_csv('./prediction/submission.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81d6e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label이 특정 label만 많이 나온다,,어떡하지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa400e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
