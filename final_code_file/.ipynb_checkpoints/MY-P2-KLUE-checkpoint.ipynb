{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RmcmqTvs1_T"
   },
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5ZMzORj6Xxn"
   },
   "source": [
    "라이브러리 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/SKTBrain/KoBERT\n",
    "# 여기 koBert랑 KoGPT2가 있는데 같이 사용해서 앙상블하면,,,좋지않을까,,,\n",
    "# 아니면 다국어 모델?도 좋을거같은데 \n",
    "# ERNIE, ELECTRA, GPT-2 이런거...? ㅎㅎ,,,,\n",
    "# KoGPT2에 토크나이저만 바꾸는건 에반가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcI3nARqs9qg"
   },
   "source": [
    "라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ETROhbNxsuXQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tarfile\n",
    "import pickle as pickle\n",
    "from tqdm import tqdm\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning import Trainer\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mahnyujin\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.26<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">elated-feather-12</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ahnyujin/Pstage2_KLUE\" target=\"_blank\">https://wandb.ai/ahnyujin/Pstage2_KLUE</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ahnyujin/Pstage2_KLUE/runs/2qk0vjnj\" target=\"_blank\">https://wandb.ai/ahnyujin/Pstage2_KLUE/runs/2qk0vjnj</a><br/>\n",
       "                Run data is saved locally in <code>/opt/ml/code/wandb/run-20210418_070350-2qk0vjnj</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login()\n",
    "wandb.init(project = 'Pstage2_KLUE', reinit = True)\n",
    "wandb.run.name = 'koBert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "seed = 210\n",
    "seed_everything(210)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcobKDe9tAuQ"
   },
   "source": [
    "GPU 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "i8v0khrlswNx"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hZw_ITPtCgp"
   },
   "source": [
    "kobert 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nhsub2pBsx1q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n",
      "Vocab(size=8002, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")\n"
     ]
    }
   ],
   "source": [
    "bertmodel, vocab = get_pytorch_kobert_model()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9_lv7GMtE1_"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "5mr-nvcjOzLF"
   },
   "outputs": [],
   "source": [
    "# def load_data(dataset_dir):\n",
    "#     with open('/opt/ml/input/data/label_type.pkl', 'rb') as f:\n",
    "#         label_type = pickle.load(f)\n",
    "#     dataset = pd.read_csv(dataset_dir, delimiter='\\t', header=None)\n",
    "#     dataset = preprocessing_dataset(dataset, label_type)\n",
    "#     return dataset\n",
    "\n",
    "# def preprocessing_dataset(dataset, label_type):\n",
    "#     label = []\n",
    "#     for i in dataset[8]:\n",
    "#         if i == 'blind':\n",
    "#             label.append(100)\n",
    "#         else:\n",
    "#             label.append(label_type[i])\n",
    "#     out_dataset = pd.DataFrame({'sentence':dataset[1],'entity_01':dataset[2],'entity_02':dataset[5],'label':label,})\n",
    "#     return out_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xkI-E7PauxGq"
   },
   "outputs": [],
   "source": [
    "# dataset_path = r\"/opt/ml/input/data/train/train+all.tsv\"\n",
    "\n",
    "# dataset = load_data(dataset_path)\n",
    "\n",
    "# dataset['sentence'] = dataset['entity_01'] + ' [SEP] ' + dataset['entity_02'] + ' [SEP] ' + dataset['sentence']\n",
    "# dataset['sentence'] += \"이 문장에서 \" + dataset['entity_01'] + \"과 \" +dataset['entity_02] + \"은 어떤 관계일까?\" + \"[SEP]\"\n",
    "# for i in glob(\"/opt/ml/input/data/train/plus/*.tsv\"):\n",
    "#     tmp = pd.read_csv(i, delimiter = '\\t')\n",
    "#     tmp['sentence'] = tmp['entity_01'] + ' [SEP] ' + tmp['entity_02'] + ' [SEP] ' + tmp['sentence']\n",
    "#     tmp['sentence'] += \"이 문장에서 \" + tmp['entity_01'] + \"과 \" +tmp['entity_02] + \"은 어떤 관계일까?\" + \"[SEP]\"\n",
    "#     dataset = pd.concat([dataset, tmp], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YIE_tnYq6AYL"
   },
   "outputs": [],
   "source": [
    "# dataset[['sentence', 'label']].to_csv(\"/opt/ml/input/data/train/train.txt\", sep='\\t', index=False)\n",
    "# train, vali = train_test_split(dataset, test_size=0.1, random_state=42)\n",
    "# train[['sentence','label']].to_csv(\"/opt/ml/input/data/train/train_train.txt\", sep='\\t',index=False) # sep='\\t',\n",
    "# vali[['sentence','label']].to_csv(\"/opt/ml/input/data/train/train_vali.txt\",  sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "id": "2tI-jupiCwpE"
   },
   "outputs": [],
   "source": [
    "# dataset_train = nlp.data.TSVDataset(\"/opt/ml/input/data/train/train_train.txt\", field_indices=[0,1], num_discard_samples=1, allow_missing = True)\n",
    "# dataset_vali = nlp.data.TSVDataset(\"/opt/ml/input/data/train/train_vali.txt\", field_indices=[0,1], num_discard_samples=1,allow_missing = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = pd.read_csv(\"/opt/ml/input/data/train/train_train.txt\", sep='\\t')\n",
    "dataset_vali = pd.read_csv(\"/opt/ml/input/data/train/train_vali.txt\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ca54j41sN-0L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "eRRaHwF_C28c"
   },
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, bert_tokenizer, max_len, pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "        try:\n",
    "            self.sentences = [transform([i]) for i in dataset['sentence']]\n",
    "        except:\n",
    "             print(i)\n",
    "        try:\n",
    "            self.labels = [np.int32(i) for i in dataset['label']]\n",
    "        except:\n",
    "            print(i)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "4BKznxZotPrl"
   },
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "batch_size = 1000\n",
    "warmup_ratio = 0.01\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 50\n",
    "learning_rate = 5e-5\n",
    "\n",
    "wandb.config.update({\n",
    "    \"model\" : \"koBERT\",\n",
    "    \"loss\" : \"focal_loss\",\n",
    "    \"max_len\" : 128,\n",
    "    \"batch_size\" : 1000,\n",
    "    \"warmup_ratio\" : 0.01,\n",
    "    \"num_epochs\" : 5,\n",
    "    \"max_grad_norm\" : 1,\n",
    "    \"log_interval\" : 50,\n",
    "    \"learning_rate\" : 5e-5\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "WtW5knVCC6ZC"
   },
   "outputs": [],
   "source": [
    "data_train = BERTDataset(dataset_train,tok, max_len, True, False)\n",
    "data_vali = BERTDataset(dataset_vali,tok, max_len, True, False)\n",
    "# print(data_train[0])\n",
    "# [0][0] : token_ids : Token Embedding을 생성하는데 필요\n",
    "# [0][1] : valid_len : Positional Embedding을 위한것\n",
    "# [0][2] : token_types : Sentence Embedding\n",
    "# token_ids, valid_length, segment_ids, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "spDs0h8tC7fX"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "vali_dataloader = torch.utils.data.DataLoader(data_vali, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0I1L7EVtShS"
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "eR9IqXuStUbL"
   },
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes = 42,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "piJyyUoutWWt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<wandb.wandb_torch.TorchGraph at 0x7f25eb2c25d0>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "TqaRnWqwtXii"
   },
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "YYExV_Uwqdpi"
   },
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes=42, smoothing=0.2, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "    \n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None,\n",
    "                 gamma=2., reduction='mean'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "        log_prob = F.log_softmax(input_tensor, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            ((1 - prob) ** self.gamma) * log_prob,\n",
    "            target_tensor,\n",
    "            weight=self.weight,\n",
    "            reduction=self.reduction\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "SvLPsHAMtYp4"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = FocalLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "wJrYbrK5taVC"
   },
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "4PDk3f8ctasE"
   },
   "outputs": [],
   "source": [
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "7uxhVAqWtcbJ"
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASK6KHOTtd2H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 3.5425219535827637 train acc 0.03125\n",
      "epoch 1 batch id 51 loss 3.4057750701904297 train acc 0.03553921568627451\n",
      "epoch 1 batch id 101 loss 3.09694766998291 train acc 0.11045792079207921\n",
      "epoch 1 batch id 151 loss 2.8024542331695557 train acc 0.16618377483443708\n",
      "epoch 1 batch id 201 loss 2.5043041706085205 train acc 0.18781094527363185\n",
      "epoch 1 batch id 251 loss 1.7319585084915161 train acc 0.22124003984063745\n",
      "epoch 1 batch id 301 loss 1.387778878211975 train acc 0.2716985049833887\n",
      "epoch 1 batch id 351 loss 1.3998737335205078 train acc 0.318019943019943\n",
      "epoch 1 batch id 401 loss 0.8158465027809143 train acc 0.3594918952618454\n",
      "epoch 1 batch id 451 loss 0.7810208201408386 train acc 0.3938470066518847\n",
      "epoch 1 batch id 501 loss 0.4365711808204651 train acc 0.42240518962075846\n",
      "epoch 1 batch id 551 loss 0.29416361451148987 train acc 0.4469147005444646\n",
      "epoch 1 batch id 601 loss 0.4826326072216034 train acc 0.46864600665557404\n",
      "epoch 1 batch id 651 loss 0.6519556641578674 train acc 0.4870871735791091\n",
      "epoch 1 batch id 701 loss 0.21648813784122467 train acc 0.5031205420827389\n",
      "epoch 1 batch id 751 loss 0.8445808291435242 train acc 0.517809587217044\n",
      "epoch 1 batch id 801 loss 0.1749468296766281 train acc 0.5323033707865169\n",
      "epoch 1 batch id 851 loss 0.25841379165649414 train acc 0.5444330199764983\n",
      "epoch 1 batch id 901 loss 0.36180633306503296 train acc 0.5539331298557159\n",
      "epoch 1 batch id 951 loss 0.24952244758605957 train acc 0.5640444269190326\n",
      "epoch 1 batch id 1001 loss 0.4236856698989868 train acc 0.5734265734265734\n",
      "epoch 1 batch id 1051 loss 0.29681047797203064 train acc 0.5817078972407231\n",
      "epoch 1 batch id 1101 loss 0.5663533210754395 train acc 0.5884423251589465\n",
      "epoch 1 batch id 1151 loss 0.3216751515865326 train acc 0.5961392267593397\n",
      "epoch 1 batch id 1201 loss 0.16923856735229492 train acc 0.6028049542048293\n",
      "epoch 1 batch id 1251 loss 0.17999695241451263 train acc 0.6083383293365308\n",
      "epoch 1 batch id 1301 loss 0.29941511154174805 train acc 0.6136385472713297\n",
      "epoch 1 batch id 1351 loss 0.2536742091178894 train acc 0.6190784603997039\n",
      "epoch 1 batch id 1401 loss 0.2608054578304291 train acc 0.6241300856531049\n",
      "epoch 1 batch id 1451 loss 0.20600557327270508 train acc 0.6278859407305307\n",
      "epoch 1 batch id 1501 loss 0.20465831458568573 train acc 0.6314748500999334\n",
      "epoch 1 batch id 1551 loss 0.29089128971099854 train acc 0.6356987427466151\n",
      "epoch 1 batch id 1601 loss 0.4511043131351471 train acc 0.6402248594628357\n",
      "epoch 1 batch id 1651 loss 0.41998034715652466 train acc 0.6434357964869776\n",
      "epoch 1 batch id 1701 loss 0.36678314208984375 train acc 0.6461456496178718\n",
      "epoch 1 batch id 1751 loss 0.3490195572376251 train acc 0.6497180182752713\n",
      "epoch 1 batch id 1801 loss 0.23548807203769684 train acc 0.6524847307051638\n",
      "epoch 1 batch id 1851 loss 0.17828354239463806 train acc 0.6550175580767152\n",
      "epoch 1 batch id 1901 loss 0.22126862406730652 train acc 0.658025381378222\n",
      "epoch 1 batch id 1951 loss 0.25361502170562744 train acc 0.6607348795489493\n",
      "epoch 1 batch id 2001 loss 0.14372630417346954 train acc 0.6634807596201899\n",
      "epoch 1 batch id 2051 loss 0.8938979506492615 train acc 0.6662755972696246\n",
      "epoch 1 batch id 2101 loss 0.3177226781845093 train acc 0.667836744407425\n",
      "epoch 1 batch id 2151 loss 0.18189136683940887 train acc 0.6700807763830776\n",
      "epoch 1 batch id 2201 loss 0.17683695256710052 train acc 0.6720666742389823\n",
      "epoch 1 batch id 2251 loss 0.13615231215953827 train acc 0.6744780097734341\n",
      "epoch 1 batch id 2301 loss 0.25578463077545166 train acc 0.6763906996957845\n",
      "epoch 1 batch id 2351 loss 0.23074869811534882 train acc 0.6787138451722671\n",
      "epoch 1 batch id 2401 loss 0.088477224111557 train acc 0.6804196168263223\n",
      "epoch 1 batch id 2451 loss 0.27865564823150635 train acc 0.682094043247654\n",
      "epoch 1 batch id 2501 loss 0.1663239300251007 train acc 0.6835640743702519\n",
      "epoch 1 batch id 2551 loss 0.3472440838813782 train acc 0.684951979615837\n",
      "epoch 1 batch id 2601 loss 0.17650015652179718 train acc 0.6866469627066513\n",
      "epoch 1 batch id 2651 loss 0.25363314151763916 train acc 0.6878890041493776\n",
      "epoch 1 batch id 2701 loss 0.1282106637954712 train acc 0.6892470381340244\n",
      "epoch 1 batch id 2751 loss 0.30280593037605286 train acc 0.6906693020719739\n",
      "epoch 1 batch id 2801 loss 0.26224568486213684 train acc 0.692152356301321\n",
      "epoch 1 batch id 2851 loss 0.1575625240802765 train acc 0.6938793405822519\n",
      "epoch 1 batch id 2901 loss 0.15490446984767914 train acc 0.6952128576352982\n",
      "epoch 1 batch id 2951 loss 0.19004106521606445 train acc 0.6968400542189088\n",
      "epoch 1 batch id 3001 loss 0.35818979144096375 train acc 0.6980381539486837\n",
      "epoch 1 batch id 3051 loss 0.36824867129325867 train acc 0.6992379547689282\n",
      "epoch 1 batch id 3101 loss 0.24436555802822113 train acc 0.7004192196065785\n",
      "epoch 1 batch id 3151 loss 0.35943886637687683 train acc 0.7012952237384957\n",
      "epoch 1 batch id 3201 loss 0.29055550694465637 train acc 0.7023098250546704\n",
      "epoch 1 batch id 3251 loss 0.14831481873989105 train acc 0.7033893417410028\n",
      "epoch 1 batch id 3301 loss 0.2075144201517105 train acc 0.7043225537715844\n",
      "epoch 1 batch id 3351 loss 0.18206624686717987 train acc 0.7053678006565205\n",
      "epoch 1 batch id 3401 loss 0.16011132299900055 train acc 0.7064925757130256\n",
      "epoch 1 batch id 3451 loss 0.23738954961299896 train acc 0.7075394813097653\n",
      "epoch 1 batch id 3501 loss 0.14416643977165222 train acc 0.7085921879463011\n",
      "epoch 1 batch id 3551 loss 0.10746274143457413 train acc 0.7092456350323852\n",
      "epoch 1 batch id 3601 loss 0.1851314753293991 train acc 0.710123923910025\n",
      "epoch 1 batch id 3651 loss 0.26788732409477234 train acc 0.7110380717611613\n",
      "epoch 1 batch id 3701 loss 0.29339027404785156 train acc 0.711750202647933\n",
      "epoch 1 batch id 3751 loss 0.36819371581077576 train acc 0.7125349906691549\n",
      "epoch 1 batch id 3801 loss 0.1321549415588379 train acc 0.7132991318074191\n",
      "epoch 1 batch id 3851 loss 0.34680694341659546 train acc 0.714100233705531\n",
      "epoch 1 batch id 3901 loss 0.1891024112701416 train acc 0.7150169828249167\n",
      "epoch 1 batch id 3951 loss 0.13296057283878326 train acc 0.7156099721589471\n",
      "epoch 1 batch id 4001 loss 0.2200227826833725 train acc 0.716399025243689\n",
      "epoch 1 batch id 4051 loss 0.1600361317396164 train acc 0.7171454579116268\n",
      "epoch 1 batch id 4101 loss 0.1316470056772232 train acc 0.7178432089734211\n",
      "epoch 1 batch id 4151 loss 0.3631153106689453 train acc 0.7184714526620092\n",
      "epoch 1 batch id 4201 loss 0.15690727531909943 train acc 0.7190252320875982\n",
      "epoch 1 batch id 4251 loss 0.20896905660629272 train acc 0.719617442954599\n",
      "epoch 1 batch id 4301 loss 0.09467248618602753 train acc 0.7202612764473378\n",
      "epoch 1 batch id 4351 loss 0.08051440864801407 train acc 0.7210195931969662\n",
      "epoch 1 batch id 4401 loss 0.4667975604534149 train acc 0.721689672801636\n",
      "epoch 1 batch id 4451 loss 0.3350133001804352 train acc 0.7222955515614469\n",
      "epoch 1 batch id 4501 loss 0.2133689671754837 train acc 0.7230684847811597\n",
      "epoch 1 batch id 4551 loss 0.18173843622207642 train acc 0.72363216875412\n",
      "epoch 1 batch id 4601 loss 0.1452266275882721 train acc 0.7243126494240383\n",
      "epoch 1 batch id 4651 loss 0.3685368299484253 train acc 0.7250188131584605\n",
      "epoch 1 batch id 4701 loss 0.48132938146591187 train acc 0.7255969474579876\n",
      "epoch 1 batch id 4751 loss 0.21282058954238892 train acc 0.726156335508314\n",
      "epoch 1 batch id 4801 loss 0.2362777590751648 train acc 0.7266585086440325\n",
      "epoch 1 batch id 4851 loss 0.3129655718803406 train acc 0.7274080086580087\n",
      "epoch 1 batch id 4901 loss 0.17113487422466278 train acc 0.7278871658845134\n",
      "epoch 1 batch id 4951 loss 0.1910596489906311 train acc 0.7284260755402949\n",
      "epoch 1 batch id 5001 loss 0.21655496954917908 train acc 0.7289417116576684\n",
      "epoch 1 batch id 5051 loss 0.13764822483062744 train acc 0.7292244110077213\n",
      "epoch 1 batch id 5101 loss 0.12685126066207886 train acc 0.7299304058027838\n",
      "epoch 1 batch id 5151 loss 0.1466796100139618 train acc 0.7303739565132984\n",
      "epoch 1 batch id 5201 loss 0.25992852449417114 train acc 0.7308690636416074\n",
      "epoch 1 batch id 5251 loss 0.12795233726501465 train acc 0.7312238145115216\n",
      "epoch 1 batch id 5301 loss 0.05132155865430832 train acc 0.731878419166195\n",
      "epoch 1 batch id 5351 loss 0.12421203404664993 train acc 0.7321937488319941\n",
      "epoch 1 batch id 5401 loss 0.1784420758485794 train acc 0.7325842436585818\n",
      "epoch 1 batch id 5451 loss 0.23991352319717407 train acc 0.7330535681526326\n",
      "epoch 1 batch id 5501 loss 0.11501716077327728 train acc 0.7334632339574623\n",
      "epoch 1 batch id 5551 loss 0.19766904413700104 train acc 0.7340907043775896\n",
      "epoch 1 batch id 5601 loss 0.4239945709705353 train acc 0.7345786466702374\n",
      "epoch 1 batch id 5651 loss 0.09783319383859634 train acc 0.7350468943549814\n",
      "epoch 1 batch id 5701 loss 0.2538289427757263 train acc 0.7354356691808455\n",
      "epoch 1 batch id 5751 loss 0.09211912006139755 train acc 0.7357633455051296\n",
      "epoch 1 batch id 5801 loss 0.08706164360046387 train acc 0.7362254352697811\n",
      "epoch 1 batch id 5851 loss 0.08441013097763062 train acc 0.7368398564347974\n",
      "epoch 1 batch id 5901 loss 0.1860329955816269 train acc 0.7372796983562108\n",
      "epoch 1 batch id 5951 loss 0.17355401813983917 train acc 0.7378749369853806\n",
      "epoch 1 batch id 6001 loss 0.15322014689445496 train acc 0.7382519580069988\n",
      "epoch 1 batch id 6051 loss 0.10978295654058456 train acc 0.7387053792761527\n",
      "epoch 1 batch id 6101 loss 0.34977343678474426 train acc 0.7391206359613178\n",
      "epoch 1 batch id 6151 loss 0.10949580371379852 train acc 0.7397069582181759\n",
      "epoch 1 batch id 6201 loss 0.14828510582447052 train acc 0.7401578374455733\n",
      "epoch 1 batch id 6251 loss 0.12274006009101868 train acc 0.7406215005599104\n",
      "epoch 1 batch id 6301 loss 0.18875457346439362 train acc 0.7411571575940327\n",
      "epoch 1 batch id 6351 loss 0.11776013672351837 train acc 0.7417631081719415\n",
      "epoch 1 batch id 6401 loss 0.2241004854440689 train acc 0.7422180128104984\n",
      "epoch 1 batch id 6451 loss 0.1281907856464386 train acc 0.7426125794450473\n",
      "epoch 1 batch id 6501 loss 0.07987181097269058 train acc 0.7431116366712813\n",
      "epoch 1 batch id 6551 loss 0.07427044212818146 train acc 0.7435696840177072\n",
      "epoch 1 batch id 6601 loss 0.16851937770843506 train acc 0.7438550977124678\n",
      "epoch 1 batch id 6651 loss 0.19168102741241455 train acc 0.7442724778228837\n",
      "epoch 1 batch id 6701 loss 0.1723739504814148 train acc 0.7446136770631249\n",
      "epoch 1 batch id 6751 loss 0.11780054122209549 train acc 0.7449174196415346\n",
      "epoch 1 batch id 6801 loss 0.1089835837483406 train acc 0.7453545434494927\n",
      "epoch 1 batch id 6851 loss 0.3171423077583313 train acc 0.7457716026857393\n",
      "epoch 1 batch id 6901 loss 0.2621247470378876 train acc 0.7461237501811332\n",
      "epoch 1 batch id 6951 loss 0.23745277523994446 train acc 0.7464528485110056\n",
      "epoch 1 batch id 7001 loss 0.3373287618160248 train acc 0.746714755034995\n",
      "epoch 1 batch id 7051 loss 0.09917983412742615 train acc 0.7470660190043965\n",
      "epoch 1 batch id 7101 loss 0.28148993849754333 train acc 0.7475839670468948\n",
      "epoch 1 batch id 7151 loss 0.07786879688501358 train acc 0.747876171164872\n",
      "epoch 1 batch id 7201 loss 0.237702876329422 train acc 0.7484073392584363\n",
      "epoch 1 batch id 7251 loss 0.44406306743621826 train acc 0.7488277478968418\n",
      "epoch 1 batch id 7301 loss 0.24978028237819672 train acc 0.7491567935899192\n",
      "epoch 1 batch id 7351 loss 0.3872832953929901 train acc 0.749464358590668\n",
      "epoch 1 batch id 7401 loss 0.19908976554870605 train acc 0.7497508782596947\n",
      "epoch 1 batch id 7451 loss 0.3050711452960968 train acc 0.7500587169507449\n",
      "epoch 1 batch id 7501 loss 0.2951614558696747 train acc 0.7504332755632582\n",
      "epoch 1 batch id 7551 loss 0.39495599269866943 train acc 0.7507201032975764\n",
      "epoch 1 batch id 7601 loss 0.1240699365735054 train acc 0.7509949348769899\n",
      "epoch 1 batch id 7651 loss 0.15590490400791168 train acc 0.7512539210560711\n",
      "epoch 1 batch id 7701 loss 0.1752096265554428 train acc 0.7516475133099597\n",
      "epoch 1 batch id 7751 loss 0.059527114033699036 train acc 0.7519231389498129\n",
      "epoch 1 batch id 7801 loss 0.3106363117694855 train acc 0.7522272785540315\n",
      "epoch 1 batch id 7851 loss 0.5207160711288452 train acc 0.7525156031078843\n",
      "epoch 1 batch id 7901 loss 0.16970926523208618 train acc 0.752693488166055\n",
      "epoch 1 batch id 7951 loss 0.10855145007371902 train acc 0.7529831153314048\n",
      "epoch 1 batch id 8001 loss 0.12689615786075592 train acc 0.7533394263217098\n",
      "epoch 1 batch id 8051 loss 0.20476102828979492 train acc 0.7536292075518569\n",
      "epoch 1 batch id 8101 loss 0.33560267090797424 train acc 0.753969417355882\n",
      "**************************************************\n",
      "epoch 1 train acc 0.7541624990494908\n",
      "**************************************************\n",
      "**************************************************\n",
      "epoch 1 test acc 0.8035545986350341\n",
      "**************************************************\n",
      "==================================================\n",
      "best_acc : 0.8035545986350341\n",
      "==================================================\n",
      "epoch 2 batch id 1 loss 0.08540960401296616 train acc 0.90625\n",
      "epoch 2 batch id 51 loss 0.6223094463348389 train acc 0.8118872549019608\n",
      "epoch 2 batch id 101 loss 0.11442413926124573 train acc 0.8029084158415841\n",
      "epoch 2 batch id 151 loss 0.1646716445684433 train acc 0.804635761589404\n",
      "epoch 2 batch id 201 loss 0.415585458278656 train acc 0.7999067164179104\n",
      "epoch 2 batch id 251 loss 0.12434396147727966 train acc 0.7983067729083665\n",
      "epoch 2 batch id 301 loss 0.4895848035812378 train acc 0.7983803986710963\n",
      "epoch 2 batch id 351 loss 0.27273029088974 train acc 0.7967414529914529\n",
      "epoch 2 batch id 401 loss 0.3122839033603668 train acc 0.7958229426433915\n",
      "epoch 2 batch id 451 loss 0.3329492509365082 train acc 0.7962167405764967\n",
      "epoch 2 batch id 501 loss 0.17392195761203766 train acc 0.7971556886227545\n",
      "epoch 2 batch id 551 loss 0.12435774505138397 train acc 0.7967899274047187\n",
      "epoch 2 batch id 601 loss 0.1237102821469307 train acc 0.7976809484193012\n",
      "epoch 2 batch id 651 loss 0.32333874702453613 train acc 0.7984350998463902\n",
      "epoch 2 batch id 701 loss 0.11471040546894073 train acc 0.7985912981455064\n",
      "epoch 2 batch id 751 loss 0.2796313166618347 train acc 0.7991428095872171\n",
      "epoch 2 batch id 801 loss 0.09360775351524353 train acc 0.800522784019975\n",
      "epoch 2 batch id 851 loss 0.14378799498081207 train acc 0.80060223266745\n",
      "epoch 2 batch id 901 loss 0.32198184728622437 train acc 0.8000138734739178\n",
      "epoch 2 batch id 951 loss 0.11414538323879242 train acc 0.8008017875920084\n",
      "epoch 2 batch id 1001 loss 0.27030980587005615 train acc 0.8016983016983017\n",
      "epoch 2 batch id 1051 loss 0.10351120680570602 train acc 0.8024500475737393\n",
      "epoch 2 batch id 1101 loss 0.35878825187683105 train acc 0.8016292007266121\n",
      "epoch 2 batch id 1151 loss 0.20276126265525818 train acc 0.8019113814074718\n",
      "epoch 2 batch id 1201 loss 0.12212537974119186 train acc 0.80261240632806\n",
      "epoch 2 batch id 1251 loss 0.11109042912721634 train acc 0.8027328137490009\n",
      "epoch 2 batch id 1301 loss 0.18997876346111298 train acc 0.8027238662567256\n",
      "epoch 2 batch id 1351 loss 0.13838109374046326 train acc 0.8031782013323464\n",
      "epoch 2 batch id 1401 loss 0.1409684270620346 train acc 0.8038008565310493\n",
      "epoch 2 batch id 1451 loss 0.20527853071689606 train acc 0.8036268090971743\n",
      "epoch 2 batch id 1501 loss 0.14146287739276886 train acc 0.803276982011992\n",
      "epoch 2 batch id 1551 loss 0.3599996566772461 train acc 0.8035340103159252\n",
      "epoch 2 batch id 1601 loss 0.3366641402244568 train acc 0.804419113054341\n",
      "epoch 2 batch id 1651 loss 0.15637865662574768 train acc 0.8040960024227741\n",
      "epoch 2 batch id 1701 loss 0.26380306482315063 train acc 0.8037184009406232\n",
      "epoch 2 batch id 1751 loss 0.08619291335344315 train acc 0.8039870074243289\n",
      "epoch 2 batch id 1801 loss 0.23556078970432281 train acc 0.8039804275402554\n",
      "epoch 2 batch id 1851 loss 0.13520273566246033 train acc 0.8042274446245273\n",
      "epoch 2 batch id 1901 loss 0.09292322397232056 train acc 0.8045436612309311\n",
      "epoch 2 batch id 1951 loss 0.12386687844991684 train acc 0.8047635827780625\n",
      "epoch 2 batch id 2001 loss 0.13120651245117188 train acc 0.8049881309345327\n",
      "epoch 2 batch id 2051 loss 0.3269445300102234 train acc 0.805110312042906\n",
      "epoch 2 batch id 2101 loss 0.1758301556110382 train acc 0.8046763445978106\n",
      "epoch 2 batch id 2151 loss 0.150639608502388 train acc 0.8045966992096699\n",
      "epoch 2 batch id 2201 loss 0.17474886775016785 train acc 0.8049040208995911\n",
      "epoch 2 batch id 2251 loss 0.1293799728155136 train acc 0.8052254553531764\n",
      "epoch 2 batch id 2301 loss 0.09755679965019226 train acc 0.8050847457627118\n",
      "epoch 2 batch id 2351 loss 0.19311963021755219 train acc 0.8051759889408763\n",
      "epoch 2 batch id 2401 loss 0.09073875844478607 train acc 0.8050031236984589\n",
      "epoch 2 batch id 2451 loss 0.12414196133613586 train acc 0.805079559363525\n",
      "epoch 2 batch id 2501 loss 0.19582535326480865 train acc 0.8052903838464615\n",
      "epoch 2 batch id 2551 loss 0.22274565696716309 train acc 0.8049906899255194\n",
      "epoch 2 batch id 2601 loss 0.0695401281118393 train acc 0.8050389273356401\n",
      "epoch 2 batch id 2651 loss 0.16599075496196747 train acc 0.8051089211618258\n",
      "epoch 2 batch id 2701 loss 0.10109331458806992 train acc 0.8049912069603851\n",
      "epoch 2 batch id 2751 loss 0.18881075084209442 train acc 0.8049913667757179\n",
      "epoch 2 batch id 2801 loss 0.17238575220108032 train acc 0.8054154766154945\n",
      "epoch 2 batch id 2851 loss 0.152151420712471 train acc 0.8054739565064889\n",
      "epoch 2 batch id 2901 loss 0.05126124992966652 train acc 0.8054119269217511\n",
      "epoch 2 batch id 2951 loss 0.1408364325761795 train acc 0.805775584547611\n",
      "epoch 2 batch id 3001 loss 0.20719514787197113 train acc 0.8059292735754748\n",
      "epoch 2 batch id 3051 loss 0.16523519158363342 train acc 0.8062418059652573\n",
      "epoch 2 batch id 3101 loss 0.11501576006412506 train acc 0.8065442599161561\n",
      "epoch 2 batch id 3151 loss 0.1919943392276764 train acc 0.8064503332275468\n",
      "epoch 2 batch id 3201 loss 0.08823791891336441 train acc 0.8065838800374883\n",
      "epoch 2 batch id 3251 loss 0.19607388973236084 train acc 0.8067037065518302\n",
      "epoch 2 batch id 3301 loss 0.07586872577667236 train acc 0.8065264313844289\n",
      "epoch 2 batch id 3351 loss 0.24146610498428345 train acc 0.8068393763055804\n",
      "epoch 2 batch id 3401 loss 0.1981772929430008 train acc 0.8069409732431638\n",
      "epoch 2 batch id 3451 loss 0.07140828669071198 train acc 0.8070305708490293\n",
      "epoch 2 batch id 3501 loss 0.13240711390972137 train acc 0.8070283490431306\n",
      "epoch 2 batch id 3551 loss 0.07722000777721405 train acc 0.8070173894677556\n",
      "epoch 2 batch id 3601 loss 0.05318943411111832 train acc 0.8070414468203276\n",
      "epoch 2 batch id 3651 loss 0.18421423435211182 train acc 0.8072103533278554\n",
      "epoch 2 batch id 3701 loss 0.12228057533502579 train acc 0.8070707241286139\n",
      "epoch 2 batch id 3751 loss 0.12276526540517807 train acc 0.8069514796054386\n",
      "epoch 2 batch id 3801 loss 0.13325954973697662 train acc 0.806868258353065\n",
      "epoch 2 batch id 3851 loss 0.15329185128211975 train acc 0.8069576084133991\n",
      "epoch 2 batch id 3901 loss 0.14297056198120117 train acc 0.8071007433991284\n",
      "epoch 2 batch id 3951 loss 0.09593529999256134 train acc 0.8070978866109846\n",
      "epoch 2 batch id 4001 loss 0.1004035621881485 train acc 0.8071966383404149\n",
      "epoch 2 batch id 4051 loss 0.12266066670417786 train acc 0.8072312392001975\n",
      "epoch 2 batch id 4101 loss 0.13293536007404327 train acc 0.8073945379175811\n",
      "epoch 2 batch id 4151 loss 0.2580983638763428 train acc 0.8075087328354613\n",
      "epoch 2 batch id 4201 loss 0.12383703142404556 train acc 0.8072705903356344\n",
      "epoch 2 batch id 4251 loss 0.2487904280424118 train acc 0.8071850741002117\n",
      "epoch 2 batch id 4301 loss 0.06124836578965187 train acc 0.8072177981864682\n",
      "epoch 2 batch id 4351 loss 0.07257810980081558 train acc 0.80725695242473\n",
      "epoch 2 batch id 4401 loss 0.36837053298950195 train acc 0.8072029084299023\n",
      "epoch 2 batch id 4451 loss 0.3415825664997101 train acc 0.8072062457874635\n",
      "epoch 2 batch id 4501 loss 0.3841887414455414 train acc 0.8073136525216619\n",
      "epoch 2 batch id 4551 loss 0.10067110508680344 train acc 0.8071783673917821\n",
      "epoch 2 batch id 4601 loss 0.10292577743530273 train acc 0.8071139426211693\n",
      "epoch 2 batch id 4651 loss 0.3582361340522766 train acc 0.8071516878090733\n",
      "epoch 2 batch id 4701 loss 0.25518798828125 train acc 0.8073082854711764\n",
      "epoch 2 batch id 4751 loss 0.32602599263191223 train acc 0.8074484319090718\n",
      "epoch 2 batch id 4801 loss 0.0716680958867073 train acc 0.8074034055405124\n",
      "epoch 2 batch id 4851 loss 0.06977541744709015 train acc 0.8076105442176871\n",
      "epoch 2 batch id 4901 loss 0.14229588210582733 train acc 0.8077050601917976\n",
      "epoch 2 batch id 4951 loss 0.13867633044719696 train acc 0.8076335588769945\n",
      "epoch 2 batch id 5001 loss 0.1133844256401062 train acc 0.8075384923015397\n",
      "epoch 2 batch id 5051 loss 0.11998505890369415 train acc 0.8073525044545634\n",
      "epoch 2 batch id 5101 loss 0.07190162688493729 train acc 0.8074642227014311\n",
      "epoch 2 batch id 5151 loss 0.13570323586463928 train acc 0.8076041059988351\n",
      "epoch 2 batch id 5201 loss 0.20895986258983612 train acc 0.8078134012689867\n",
      "epoch 2 batch id 5251 loss 0.0892409086227417 train acc 0.8077866120738907\n",
      "epoch 2 batch id 5301 loss 0.03444526344537735 train acc 0.8079607621203546\n",
      "epoch 2 batch id 5351 loss 0.08224087208509445 train acc 0.8079330966174547\n",
      "epoch 2 batch id 5401 loss 0.17301462590694427 train acc 0.8079985187928161\n",
      "epoch 2 batch id 5451 loss 0.2287137806415558 train acc 0.8079996789579894\n",
      "epoch 2 batch id 5501 loss 0.08337301760911942 train acc 0.8079951372477732\n",
      "epoch 2 batch id 5551 loss 0.08990290015935898 train acc 0.8081820843091335\n",
      "epoch 2 batch id 5601 loss 0.33730703592300415 train acc 0.8081313604713444\n",
      "epoch 2 batch id 5651 loss 0.08115477114915848 train acc 0.8081534241727127\n",
      "epoch 2 batch id 5701 loss 0.11026042699813843 train acc 0.8081915453429223\n",
      "epoch 2 batch id 5751 loss 0.08487707376480103 train acc 0.8080007824726134\n",
      "epoch 2 batch id 5801 loss 0.054519567638635635 train acc 0.8080718841579038\n",
      "epoch 2 batch id 5851 loss 0.08058875799179077 train acc 0.808237908049906\n",
      "epoch 2 batch id 5901 loss 0.13342808187007904 train acc 0.808390527029317\n",
      "epoch 2 batch id 5951 loss 0.1244032084941864 train acc 0.8086403545622585\n",
      "epoch 2 batch id 6001 loss 0.13129045069217682 train acc 0.8085058740209965\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    best_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "            wandb.log({\n",
    "                \"loss\" :  loss.data.cpu().numpy(),\n",
    "                \"train acc \" : train_acc / (batch_id+1)\n",
    "            })\n",
    "    print('*'*50)\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    wandb.log({\n",
    "        \"train acc \" :  train_acc / (batch_id+1)\n",
    "    })\n",
    "    print('*'*50)\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(vali_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length = valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print('*'*50)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "    wandb.log({\n",
    "        \"test acc\" : test_acc / (batch_id+1)\n",
    "    })\n",
    "    print('*'*50)\n",
    "    if test_acc >= best_acc:\n",
    "        best_acc = test_acc\n",
    "        print('='*50)\n",
    "        print(f\"best_acc : {best_acc / (batch_id+1)}\")\n",
    "        print('='*50)\n",
    "        torch.save(model.state_dict(), \"/opt/ml/model/model_state_dict.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7ASgrTpfdZh"
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "Siuxwi6SdiTW"
   },
   "outputs": [],
   "source": [
    "dataset_path = r\"/opt/ml/input/data/test/test.tsv\"\n",
    "\n",
    "dataset = load_data(dataset_path)\n",
    "\n",
    "dataset['sentence'] = dataset['entity_01'] + ' [SEP] ' + dataset['entity_02'] + ' [SEP] ' + dataset['sentence']\n",
    "\n",
    "dataset[['sentence','label']].to_csv(\"/opt/ml/input/data/test/test.txt\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "yPfoO4ym6AYU"
   },
   "outputs": [],
   "source": [
    "dataset_test = nlp.data.TSVDataset(\"/opt/ml/input/data/test/test.txt\", field_indices=[0,1], num_discard_samples=1)\n",
    "\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "g3TFf_YgtjDG"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"/opt/ml/model/model_state_dict.pt\"))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "Predict = []\n",
    "\n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length = valid_length\n",
    "    label = label.long().to(device)\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "    _, predict = torch.max(out,1)\n",
    "    Predict.extend(predict.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "_aV-Fgpffp4s"
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame(Predict, columns=['pred'])\n",
    "output.to_csv('/opt/ml/result/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "P2-KLUE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
